{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c252c4d8",
   "metadata": {},
   "source": [
    "# ZINC Training Dataset Setup for small molecule language models\n",
    "### Preparation of a drug-like subset of the ZINC15 dataset\n",
    "[ZINC](https://zinc15.docking.org/) is a free database of commercially-available compounds for virtual screening (*J. Chem. Inf. Model*. 2015, 55, 11, 2324–2337). In this notebook, we will walk through the steps to create a drug-like subset of the ZINC15 dataset ready for use for training or inference of BioNeMo small molecule language models such as [MolMIM](https://docs.nvidia.com/bionemo-framework/latest/models/molmim.html) and [MegaMolBART](https://docs.nvidia.com/bionemo-framework/latest/models/megamolbart.html).\n",
    "\n",
    "These steps involve:\n",
    "- input validation\n",
    "- smiles canonicalisation\n",
    "- filtering by maximum token length of input (per model constraint)\n",
    "- filtering based on tokenizer vocabulary \n",
    "- filtering by compliance to Lipinski rule of 5\n",
    "\n",
    "\n",
    "NOTE: this notebook was developed and tested for BioNeMo framework container 1.7  \n",
    "Tested GPUs: A1000, A6000 \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> Some of the cells below generate long text output.  We're using <pre>%%capture --no-display --no-stderr cell_output</pre> to suppress this output.  Comment or delete this line in the cells below to restore full output.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f89a93b",
   "metadata": {},
   "source": [
    "First import all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "620321b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "\n",
    "from nemo.collections.common.tokenizers.regex_tokenizer import RegExTokenizer\n",
    "\n",
    "bionemo_home = os.environ['BIONEMO_HOME']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15553f5",
   "metadata": {},
   "source": [
    "### 1. Download the data\n",
    "We will download ZINC data from https://files.docking.org/\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> The following cell may fail due to a certificate validation error in the wget commmand.  You can skip the certificate check using <pre> wget -O {raw_data_file} --no-check-certificate https://files.docking.org/2D/AB/ABAC.txt </pre>  This is a potential security risk; use with caution.  </div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "962563c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-03 14:41:24--  https://files.docking.org/2D/AB/ABAC.txt\n",
      "Resolving files.docking.org (files.docking.org)... 169.230.75.4\n",
      "Connecting to files.docking.org (files.docking.org)|169.230.75.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 443705 (433K) [text/plain]\n",
      "Saving to: ‘/workspace/bionemo/data/raw/ZINC_sample_raw_data.txt’\n",
      "\n",
      "/workspace/bionemo/ 100%[===================>] 433.31K   516KB/s    in 0.8s    \n",
      "\n",
      "2024-09-03 14:41:25 (516 KB/s) - ‘/workspace/bionemo/data/raw/ZINC_sample_raw_data.txt’ saved [443705/443705]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We will download just 1 file for illustration purposes\n",
    "raw_data_dir = f\"{bionemo_home}/data/raw\"\n",
    "! mkdir -p {raw_data_dir}\n",
    "raw_data_file = f\"{raw_data_dir}/ZINC_sample_raw_data.txt\"\n",
    "! wget -O {raw_data_file} https://files.docking.org/2D/AB/ABAC.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14766ea6",
   "metadata": {},
   "source": [
    "### 2. Process the data\n",
    "#### 2.1 SMILES validation and canonicalisation\n",
    "We will drop input SMILES that cannot generate a valid mol object and obtain the canonical SMILES string for each input molecule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd9179d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating canonical SMILES strings from the provided SMILES...\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv(raw_data_file, sep=\"\\t\", usecols=[\"zinc_id\", \"smiles\"])\n",
    "\n",
    "def canonicalise_smiles(smiles: str) -> str:\n",
    "    \"\"\"Returns the canonical SMILES string for the input SMILES string \n",
    "    or np.nan if it was not possible to generate a valid mol object from the input string\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    return np.nan if mol is None else Chem.MolToSmiles(mol)\n",
    "    \n",
    "print(\"Generating canonical SMILES strings from the provided SMILES...\")\n",
    "data_df[\"canonical_smiles\"] = data_df[\"smiles\"].map(canonicalise_smiles)\n",
    "data_df.dropna(subset=[\"canonical_smiles\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20ba0f9",
   "metadata": {},
   "source": [
    "#### 2.2 Filter for vocabulary compliance\n",
    "Next, we exclude SMILES strings exceeding the model's maximum token limit and molecules with tokens absent from the model's vocabulary. To accomplish this, we import the model's existing vocabulary files, using MolMIM as an illustrative example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92eb30c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-09-03 14:41:26 regex_tokenizer:240] Loading vocabulary from file = /workspace/bionemo/tokenizers/molecule/molmim/vocab/molmim.vocab\n",
      "[NeMo I 2024-09-03 14:41:26 regex_tokenizer:254] Loading regex from file = /workspace/bionemo/tokenizers/molecule/molmim/vocab/molmim.model\n"
     ]
    }
   ],
   "source": [
    "model_name = \"molmim\"\n",
    "\n",
    "# Note: the maximum token length generated from the smiles string should be 2 less than the max_seq_length specified in the model config. \n",
    "# This is to account for the extra tokens <BOS> and <EOS>\n",
    "MAX_TOKEN_LENGTH = 126\n",
    "\n",
    "# Provide path to files containing allowed vocabulary\n",
    "# Note: This file can be changed according to the user's needs\n",
    "tokenizer_path = bionemo_home + \"/tokenizers/molecule/{model_name}/vocab/{model_name}.{extension}\"\n",
    "tokenizer = RegExTokenizer().load_tokenizer(regex_file=tokenizer_path.format(model_name=model_name, extension=\"model\"), \n",
    "                                                  vocab_file=tokenizer_path.format(model_name=model_name, extension=\"vocab\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a780aa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_compliance_check(smiles: str, tokenizer: RegExTokenizer, max_token_length: int) -> bool:\n",
    "    \"\"\"Checks if the SMILES string only contains vocabulary in the tokenizer's vocabulary\n",
    "    and if the token length is less than or equal to `max_token_length\"\"\"\n",
    "    tokens = tokenizer.text_to_tokens(smiles)\n",
    "    vocab_allowed = tokenizer.vocab.keys()\n",
    "    return set(tokens).issubset(set(vocab_allowed)) and len(tokens) <= max_token_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a840a16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out molecules which are not present in the molmim tokenizer vocabulary or with max token length greater than 126...\n",
      "5 molecules removed.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Filtering out molecules which are not present in the {model_name} tokenizer vocabulary or with max token length greater than {MAX_TOKEN_LENGTH}...\")\n",
    "data_df[\"vocab_compliant\"] = data_df[\"canonical_smiles\"].apply(lambda smi: vocab_compliance_check(smi, tokenizer, MAX_TOKEN_LENGTH))\n",
    "\n",
    "# Select only molecules which are vocab compliant\n",
    "vocab_compliant_df = data_df.loc[data_df['vocab_compliant']]\n",
    "print(f\"{len(data_df) - len(vocab_compliant_df)} molecules removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f34d016",
   "metadata": {},
   "source": [
    "#### 2.3 Filter out undesirable molecules\n",
    "In this step, we filter out non-druglike molecules, where druglikeness is estimated using the following criteria:\n",
    "1. [Lipinski's rule of 5 compliance](https://www.sciencedirect.com/science/article/abs/pii/S0169409X96004231)\n",
    "2. [Quantitative Estimate of Druglikeness](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3524573/) (QED score) with a cutoff of `0.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a1a9d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating druglikeness of input molecules...\n",
      "Filtering out molecules which do not meet Lipinski and QED criteria...\n",
      "1594 molecules removed.\n"
     ]
    }
   ],
   "source": [
    "def determine_druglikeness(smiles: str, property: str) -> bool:\n",
    "    \"\"\"Calculates the specified property for the input smiles \n",
    "    and returns a boolean indicating whether the calculated property passes Lipinski's rule of 5\"\"\"\n",
    "    \n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return np.nan\n",
    "    if property == \"mol_weight\":\n",
    "        return True if Descriptors.ExactMolWt(mol) <= 500 else False\n",
    "    elif property == \"clogp\":\n",
    "        return True if Chem.Crippen.MolLogP(mol) <= 5 else False\n",
    "    elif property == \"hbd\":\n",
    "        return True if Descriptors.NumHDonors(mol) <= 5 else False\n",
    "    elif property == \"hba\":\n",
    "        return True if Descriptors.NumHAcceptors(mol) <= 10 else False\n",
    "    elif property == \"qed_score\":\n",
    "        return True if Chem.QED.qed(mol) >= 0.5 else False\n",
    "    else:\n",
    "        raise ValueError('Please choose property from the following options: [\"mol_weight\", \"clogp\",\"hbd\", \"hba\", \"qed_score\"]')\n",
    "\n",
    "druglikeness_criteria = [\"mol_weight\", \"clogp\",\"hbd\", \"hba\", \"qed_score\"]\n",
    "\n",
    "print(f\"Calculating druglikeness of input molecules...\")\n",
    "\n",
    "for property in druglikeness_criteria:\n",
    "    vocab_compliant_df.loc[:,property] = vocab_compliant_df.loc[:,\"smiles\"].map(lambda smiles: determine_druglikeness(smiles, property))\n",
    "\n",
    "print(f\"Filtering out molecules which do not meet Lipinski and QED criteria...\")\n",
    "druglike_df = vocab_compliant_df.query(\"mol_weight & clogp & hbd & hba & qed_score\")\n",
    "\n",
    "print(f\"{len(vocab_compliant_df) - len(druglike_df)} molecules removed.\")\n",
    "\n",
    "druglike_df = druglike_df[[\"zinc_id\", \"smiles\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d71bee8",
   "metadata": {},
   "source": [
    "### 3. Split dataset to create training, validation and test sets\n",
    "\n",
    "Finally, we split the dataset into training, validation, and test sets with a ratio of 8:1:1, respectively. These sets are then saved as CSV files in designated subdirectories (`train`, `val` and `test`) as required for model training. Other dataset variables e.g. columns, headers, dataset_path are specified in the config used for training, see example config files in `examples/conf/molecule` for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b2b446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_test_random_split(input_df: pd.DataFrame, test_frac: float, val_frac: float) -> dict:\n",
    "    \"\"\"Splits the input_df into train, validation and test sets randomly according to the specified fractions \n",
    "    and returns a dictionary of dataframes\n",
    "    \"\"\"\n",
    "    # Calculate sample sizes before size of dataframe changes\n",
    "    test_samples = max(int(test_frac * input_df.shape[0]), 1)\n",
    "    val_samples = max(int(val_frac * input_df.shape[0]), 1)\n",
    "\n",
    "    splits={}\n",
    "    splits[\"test\"] = input_df.sample(n=test_samples, random_state=0)\n",
    "    # remove test data from training data\n",
    "    input_df = input_df.drop(splits[\"test\"].index)  \n",
    "\n",
    "    splits[\"val\"] = input_df.sample(n=val_samples, random_state=0)\n",
    "    # remove validation data from training data\n",
    "    splits[\"train\"] = input_df.drop(splits[\"val\"].index)  \n",
    "    return splits\n",
    "\n",
    "\n",
    "test_frac = 0.1\n",
    "val_frac = 0.1\n",
    "\n",
    "splits = train_valid_test_random_split(druglike_df, test_frac, val_frac)\n",
    "\n",
    "for molecule_set in splits.keys():\n",
    "    output_dir = os.path.join(bionemo_home, f\"data/processed/{molecule_set}\")\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    splits[molecule_set].to_csv(os.path.join(output_dir, f\"{molecule_set}_set.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415fdebc",
   "metadata": {},
   "source": [
    "### 4. Example of MolMIM pretraining using the prepared data\n",
    "\n",
    "Here we will run pretraining from scratch on our prepared data for molmim as an example using the default config `pretrain_xsmall_canonicalized.yaml`. Other example configs can be found in directory `examples/molecule/molmim/conf`. We will override some config arguments at runtime using Hydra. \n",
    "\n",
    "The column containing the SMILES strings in the input datasets is specified with the argument `model.data.data_impl_kwargs.csv_mmap.data_col`. We specify the directory containing our newly created `train`, `val` and `test` subdirectories using the argument `model.data.dataset_path` and the names of our input files using argument `model.data.dataset.train` etc.\n",
    "\n",
    "Note, index files are written to the file specified in `model.data.index_mapping_dir`, if index files here are present they will be read rather than written. Therefore, we here clear the index files first to avoid unintentional errors which could occur if changing model or dataset params in this notebook playground.\n",
    "For the same reason we set `exp_manager.resume_if_exists` to be `False`, otherwise if training was interrupted and then restarted, training will continue from the last saved checkpoint, and errors could occur if model params in the config have been changed.\n",
    "\n",
    "We will also reduce the number of training steps (`trainer.max_steps`) and correspondingly the step interval for checking the validation set (`trainer.val_check_interval`) just for the purpose of this demonstration.\n",
    "\n",
    "Optional: Add arguments for logging with Weights and Biases and login when prompted \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "159cc3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "! rm -rf data/data_index\n",
    "! cd {bionemo_home} && python examples/molecule/molmim/pretrain.py \\\n",
    "    do_training=True \\\n",
    "    ++model.data.dataset_path=\"data/processed/\" \\\n",
    "    ++model.data.dataset.train=\"train_set\" \\\n",
    "    ++model.data.dataset.val=\"val_set\" \\\n",
    "    ++model.data.dataset.test=\"test_set\" \\\n",
    "    ++model.data.index_mapping_dir=\"data/data_index/\" \\\n",
    "    ++model.data.data_impl_kwargs.csv_mmap.data_col=1 \\\n",
    "    ++model.dwnstr_task_validation.enabled=False \\\n",
    "    ++model.global_batch_size=null \\\n",
    "    ++trainer.devices=1 \\\n",
    "    ++trainer.accelerator='gpu' \\\n",
    "    ++trainer.max_steps=200 \\\n",
    "    ++trainer.val_check_interval=100 \\\n",
    "    ++exp_manager.create_wandb_logger=False \\\n",
    "    ++exp_manager.resume_if_exists=False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
