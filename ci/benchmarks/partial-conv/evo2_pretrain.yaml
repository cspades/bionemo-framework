scope: partial-conv
time_limit: 14400
script_args:
  # All arguments referenced in the script string must be specified here.
  # Arguments not referenced in the script string must have the 'arg' field specified.
  # See jet/core/configs.py for the specification of the configuration class
  workspace:
    value: /workspace/bionemo2
    key_segment: False
  data_path:
    value: /data/evo2
    key_segment: False
  model:
    value: evo2
  variant:
    value: train
  config_name: 1b
  precision: fp8
  gpus: 8
  nodes: 4
  batch_size: 8
  max_steps: 490000
  stop_steps: 6900
  pp: 1
  cp: 1
  tp: 1
  seq_len: 8192
  acc_grad: 1
  clip_grad:
    value: 250
    key_segment: False
  seed: 3735928559
  lr:
    value: 0.00015
    key_segment: False
  min_lr:
    value: 0.000015
    key_segment: False
  wu_steps:
    value: 5000
    key_segment: False
  wd:
    value: 0.1
    key_segment: False
script: |-
  WANDB_API_KEY=$BIONEMO_WANDB_API_KEY python ${workspace}/sub-packages/bionemo-evo2/src/bionemo/evo2/run/${variant}.py \
  -d /workspace/bionemo2/sub-packages/bionemo-evo2/examples/configs/full_pretrain_shortphase_config.yaml \
  --dataset-dir ${data_path} \
  --grad-acc-batches ${acc_grad} \
  --fp8 --fp8-wgrad --activation-checkpoint-recompute-num-layers 5 \
  --enable-preemption \
  --ckpt-async-save \
  --use-megatron-comm-overlap-llama3-8k \
  --overlap-grad-reduce \
  --clip-grad=${clip_grad} \
  --eod-pad-in-loss-mask \
  --seq-length=${seq_len} \
  --lr=${lr} \
  --wd=${wd} \
  --min-lr=${min_lr} \
  --warmup-steps=${wu_steps} \
  --tensor-parallel-size=${tp} \
  --context-parallel-size=${cp} \
  --pipeline-model-parallel-size=${pp} \
  --workers 8 \
  --num-nodes=${nodes} \
  --devices=${gpus} \
  --micro-batch-size=${batch_size} \
  --model-size=${config_name} \
  --max-steps=${max_steps} --early-stop-on-step ${stop_steps} \
  --limit-val-batches=20 \
  --log-every-n-steps=50 \
  --val-check-interval=500 \
  --tflops-callback \
  --experiment-dir=${tensorboard_dir} \
  --wandb-project=${wandb_project_name} \
  --wandb-group=${model}_${variant}_${config_name}__${target}__slen${seq_len} \
  --wandb-job-type=${pipeline_label} \
  --disable-checkpointing;
